{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "d631c4f8937aff5a00d938bb761b79b71fe773d07f3a79c191fcf956b8a61c95"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Introduction to Artificial Intelligence on Azure\n",
    "\n",
    "![Brain with cogs](./images/ai.png)\n",
    "\n",
    "### A 30 minute exploration of AI for beginners\n",
    "\n",
    "#### Graeme Malcolm - Principle Content PM - Data Science and AI, Microsoft Worldwide Learning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## What is AI?\n",
    "\n",
    "### \"Smart\" software that exhibits human-like capabilities, such as:\n",
    "\n",
    "### &#8226; Decision making and anomaly detection\n",
    "\n",
    "### &#8226; Visual perception\n",
    "\n",
    "### &#8226; Natural language understanding\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Machine Learning\n",
    "\n",
    "### The foundation for most modern AI\n",
    "\n",
    "![Bike rentals](./images/bike-rentals.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load the training dataset\n",
    "data = pd.read_csv('./data/daily-bike-share.csv')\n",
    "\n",
    "# Display a random sample of 10 observations\n",
    "sample = data.sample(10)\n",
    "sample"
   ]
  },
  {
   "source": [
    "## Azure Machine Learning\n",
    "\n",
    "[Azure Machine Learning Studio](https://ml.azure.com)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import json\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Machine Learning service details\n",
    "load_dotenv()\n",
    "endpoint = os.getenv('BIKE_RENTALS_ENDPOINT')\n",
    "key = os.getenv('BIKE_RENTALS_KEY')\n",
    "\n",
    "# An array of features based on five-day weather forecast\n",
    "x = [[1,1,2022,1,0,6,0,2,0.344167,0.363625,0.805833,0.160446],\n",
    "    [2,1,2022,1,0,0,0,2,0.363478,0.353739,0.696087,0.248539],\n",
    "    [3,1,2022,1,0,1,1,1,0.196364,0.189405,0.437273,0.248309],\n",
    "    [4,1,2022,1,0,2,1,1,0.2,0.212122,0.590435,0.160296],\n",
    "    [5,1,2022,1,0,3,1,1,0.226957,0.22927,0.436957,0.1869]]\n",
    "\n",
    "# Convert the array to JSON format\n",
    "input_json = json.dumps({\"data\": x})\n",
    "\n",
    "# Set the content type and authentication for the request\n",
    "headers = {\"Content-Type\":\"application/json\",\n",
    "        \"Authorization\":\"Bearer \" + key}\n",
    "\n",
    "# Send the request\n",
    "response = requests.post(endpoint, input_json, headers=headers)\n",
    "\n",
    "# If we got a valid response, display the predictions\n",
    "if response.status_code == 200:\n",
    "    y = json.loads(response.json())\n",
    "    print(\"Predictions:\")\n",
    "    for i in range(len(x)):\n",
    "        print (\" Day: {}. Predicted rentals: {}\".format(i+1, max(0, round(y[\"result\"][i]))))\n",
    "else:\n",
    "    print(response)"
   ]
  },
  {
   "source": [
    "## Computer Vision\n",
    "\n",
    "### Interacting with the world through visual perception\n",
    "\n",
    "![Grocery store checkout](./images/checkout.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Custom Vision Cognitive Service\n",
    "\n",
    "[Custom Vision portal](https://customvision.ai)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from dotenv import load_dotenv\n",
    "from azure.cognitiveservices.vision.customvision.prediction import CustomVisionPredictionClient\n",
    "from msrest.authentication import ApiKeyCredentials\n",
    "from my_code import vision\n",
    "from PIL import Image\n",
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "# Custom Vision project details\n",
    "load_dotenv()\n",
    "project_id = os.getenv('CUSTOM_VISION_PROJECT_ID')\n",
    "cv_endpoint = os.getenv('CUSTOM_VISION_ENDPOINT')\n",
    "cv_key = os.getenv('CUSTOM_VISION_KEY')\n",
    "model_name = 'detect-produce'\n",
    "\n",
    "# Path to test image\n",
    "test_img_file = os.path.join('data', 'object-detection', 'produce.jpg')\n",
    "\n",
    "# Get a prediction client for the object detection model\n",
    "credentials = ApiKeyCredentials(in_headers={\"Prediction-key\": cv_key})\n",
    "predictor = CustomVisionPredictionClient(endpoint=cv_endpoint, credentials=credentials)\n",
    "\n",
    "# Detect objects in the test image\n",
    "print('Detecting objects in {} ...'.format(test_img_file))\n",
    "with open(test_img_file, mode=\"rb\") as test_data:\n",
    "    results = predictor.detect_image(project_id, model_name, test_data)\n",
    "\n",
    "# Display the image and detected objects\n",
    "img = Image.open(test_img_file)\n",
    "vision.show_detected_objects(img, results.predictions)"
   ]
  },
  {
   "source": [
    "## Natural Language Processing\n",
    "\n",
    "### Understanding written and spoken language\n",
    "\n",
    "![Home automation](./images/home-automation.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Language Understanding Service\n",
    "\n",
    "[Language Understanding portal](https://www.luis.ai)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.cognitiveservices.language.luis.runtime import LUISRuntimeClient\n",
    "from msrest.authentication import CognitiveServicesCredentials\n",
    "from dotenv import load_dotenv\n",
    "from my_code import luis\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Prepare to use the Language Understanding service\n",
    "load_dotenv()\n",
    "lu_app_id = os.getenv('LU_APP_ID')\n",
    "lu_prediction_endpoint = os.getenv('LU_PREDICTION_ENDPOINT')\n",
    "lu_prediction_key = os.getenv('LU_PREDICTION_KEY')\n",
    "credentials = CognitiveServicesCredentials(lu_prediction_key)\n",
    "lu_client = LUISRuntimeClient(lu_prediction_endpoint, credentials)\n",
    "\n",
    "# Get intent and entities for text input\n",
    "userText = input('\\nEnter a command')\n",
    "request = { \"query\" : userText }\n",
    "slot = 'Production'\n",
    "prediction_response = lu_client.prediction.get_slot_prediction(lu_app_id, slot, request)\n",
    "intent = prediction_response.prediction.top_intent\n",
    "entities = prediction_response.prediction.entities\n",
    "\n",
    "# Perform the appropriate action\n",
    "luis.show_action(intent, entities)"
   ]
  },
  {
   "source": [
    "### Speech Recognition and Synthesis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azure.cognitiveservices.speech as speech_sdk\n",
    "from dotenv import load_dotenv\n",
    "from my_code import luis\n",
    "import json\n",
    "%matplotlib inline\n",
    "\n",
    "# Prepare to use the Speech and Language Understanding Services\n",
    "load_dotenv()\n",
    "lu_app_id = os.getenv('LU_APP_ID')\n",
    "lu_prediction_region = os.getenv('LU_PREDICTION_REGION')\n",
    "lu_prediction_key = os.getenv('LU_PREDICTION_KEY')\n",
    "speech_config = speech_sdk.SpeechConfig(subscription=lu_prediction_key, region=lu_prediction_region)\n",
    "recognizer = speech_sdk.intent.IntentRecognizer(speech_config)\n",
    "model = speech_sdk.intent.LanguageUnderstandingModel(app_id=lu_app_id)\n",
    "intents = [\n",
    "    (model, \"switch_on\"),\n",
    "    (model, \"switch_off\"),\n",
    "    (model, \"None\")\n",
    "]\n",
    "recognizer.add_intents(intents)\n",
    "\n",
    "# Get intent and entities for spoken input\n",
    "print(\"What do you want me to do?\")\n",
    "result = recognizer.recognize_once_async().get()\n",
    "intent = result.intent_id\n",
    "entities = json.loads(result.intent_json)[\"entities\"]\n",
    "\n",
    "# Perform the appropriate action\n",
    "luis.show_action_from_speech(intent, entities)"
   ]
  },
  {
   "source": [
    "## Conversational AI\n",
    "\n",
    "### Engaging with *bots*\n",
    "\n",
    "![chat](./images/chat.png)\n",
    "\n",
    "## QnA Maker\n",
    "\n",
    "[QnA Maker portal](www.qnamaker.ai)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "\n",
    "<iframe src='https://webchat.botframework.com/embed/gmalc-ignite-bot?s=tP55dFBlqvo.vp7TIxL8K3M3uRMOgJcCEsWw63bFJuhbujn0ZPbL6Lk'  style='min-width: 400px; width: 100%; min-height: 300px;'></iframe>"
   ]
  },
  {
   "source": [
    "## Learn More\n",
    "\n",
    "All of the topics discussed in this session, and more, are covered in the Microsoft Azure AI Fundamentals certification track. You can learn more about the free learning options, and how to get certified at [https://docs.microsoft.com/learn/certifications/azure-ai-fundamentals/](https://docs.microsoft.com/learn/certifications/azure-ai-fundamentals/)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}