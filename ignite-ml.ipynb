{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to be a Data Scientist on Azure\n",
    "\n",
    "## or\n",
    "\n",
    "# Adventures with Pythons, Pandas, and Penguins\n",
    "\n",
    "![animals](./images/zoo.png)\n",
    "\n",
    "\n",
    "### A 30 minute exploration of data analysis and machine learning for beginners\n",
    "\n",
    "#### Graeme Malcolm - Principle Content PM - Data Science and AI, Microsoft Worldwide Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation and Exploration\n",
    "\n",
    "Let's try to see how characteristics (*features*) of penguin observations might relate to their species (*label*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1613602116345
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load the training dataset\n",
    "penguins = pd.read_csv('./data/penguin-data.csv')\n",
    "\n",
    "# Display a random sample of 10 observations\n",
    "sample = penguins.sample(10)\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's match the species IDs to the actual species names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1613602163398
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "penguin_classes = ['Amelie', 'Gentoo', 'Chinstrap']\n",
    "species_names = sample['Species'].apply(lambda x: penguin_classes[x])\n",
    "sample['SpeciesName'] = species_names\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, we need to begin by cleaning up the data. For example, are there any missing values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1613602382395
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Show rows containing nulls\n",
    "penguins[penguins.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, let's just get rid of those rows so we have nice, clean data to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1613602404829
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Drop rows containing NaN values\n",
    "penguins=penguins.dropna()\n",
    "# Confirm there are now no nulls\n",
    "penguins.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's lots we can do to explore the data. For example, let's visualize the distribution of the features for each species, so we can see how they compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1613602489167
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "penguin_features = ['CulmenLength','CulmenDepth','FlipperLength','BodyMass']\n",
    "penguin_label = 'Species'\n",
    "for col in penguin_features:\n",
    "    penguins.boxplot(column=col, by=penguin_label, figsize=(6,6))\n",
    "    plt.title(col)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there are some relationships that migth help us identify a species from the measurements:\n",
    "\n",
    "- Species 0 (Amelie) tends to have a short, but deep culmen (bill), and generally has short flippers and a low body mass.\n",
    "- Species 1 (Gentoo) tends to have medium length, thin culmen, with long flippers and a high body mass.\n",
    "- Species 2 (Chinstrap) tends to have a long and deep culmen, medium length flipper, and a low body mass.\n",
    "\n",
    "## Machine Learning\n",
    "\n",
    "Now that we know the features in the data might help differentiate the different species, we can apply an algorithm to analyze and the relationships between the features (measurements) and known labels (species), and encapsulate them in a model that can be used to predict the species for a new penguin observation based on its measurements.\n",
    "\n",
    "In machine learning terms, this kind of model is a *classification* model (because it predicts the *class* or *category* of something), and it's a form of *supervised* machine learning; which means that we use data for which we have known feature *and* label values to train a model that can predict unknown *labels* from known *features*.\n",
    "\n",
    "The first thing we'll do is reserve (or *hold-back*) some of the data. That way we can use some of the data to train the model, and then use the data we held back to test how well it predicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1613602647147
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separate features and labels\n",
    "penguins_X, penguins_y = penguins[penguin_features].values, penguins[penguin_label].values\n",
    "\n",
    "# Split data 70%-30% into training set and test set\n",
    "x_penguin_train, x_penguin_test, y_penguin_train, y_penguin_test = train_test_split(penguins_X, penguins_y,\n",
    "                                                                                    test_size=0.30,\n",
    "                                                                                    random_state=0,\n",
    "                                                                                    stratify=penguins_y)\n",
    "\n",
    "print ('Training Set: %d, Test Set: %d \\n' % (x_penguin_train.shape[0], x_penguin_test.shape[0]))\n",
    "print('\\nTraining data:')\n",
    "print('Features:')\n",
    "print(x_penguin_train[0:5])\n",
    "print('Labels:')\n",
    "print(y_penguin_train[0:5])\n",
    "print('\\nValidation data:')\n",
    "print('Features:')\n",
    "print(x_penguin_test[0:5])\n",
    "print('Labels:')\n",
    "print(y_penguin_test[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now we're ready to fit the training dataset to a classification algorithm in order to create a predictive model.\n",
    "\n",
    "In this case, we're using a logistic regression algorithm, which determines the probability of an observation belonging to each class. There are lots of algorithms you can use, and data scientists generally experiment with lots of them to find the best model for their needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1613602756991
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Set regularization rate\n",
    "reg = 0.1\n",
    "\n",
    "# train a logistic regression model on the training set\n",
    "model = LogisticRegression(C=1/reg, solver='lbfgs', multi_class='auto', max_iter=10000).fit(x_penguin_train, y_penguin_train)\n",
    "print (model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the model trained, let's see how well it predicts the classes for the test data we held back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1613603172414
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Get predictions from test data\n",
    "penguin_predictions = model.predict(x_penguin_test)\n",
    "\n",
    "# Show me the predictions!\n",
    "df_predictions = pd.DataFrame({\n",
    "    'Predicted label':penguin_predictions,\n",
    "    'Actual label':y_penguin_test\n",
    "})\n",
    "df_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's difficult to tell from just the raw predicted and actual values like this, so we generally calculate some standard metrics that quantify the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1613603282984
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from sklearn. metrics import classification_report\n",
    "\n",
    "print(classification_report(y_penguin_test, penguin_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification report includes the following metrics for each class  (0, 1, and 2)\n",
    "\n",
    "> note that the header row may not line up with the values!\n",
    "\n",
    "* *Precision*: Of the predictions the model made for this class, what proportion were correct?\n",
    "* *Recall*: Out of all of the instances of this class in the test dataset, how many did the model identify?\n",
    "* *F1-Score*: An average metric that takes both precision and recall into account.\n",
    "* *Support*: How many instances of this class are there in the test dataset?\n",
    "\n",
    "The classification report also includes overall metrics for the model as a whole."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common way to evaluate a classification model is to generate a *confusion matrix* that tabulates true and false predictions for each class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1613603437019
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Print the confusion matrix\n",
    "c_matrix = confusion_matrix(y_penguin_test, penguin_predictions)\n",
    "plt.imshow(c_matrix, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(penguin_classes))\n",
    "plt.xticks(tick_marks, penguin_classes, rotation=45)\n",
    "plt.yticks(tick_marks, penguin_classes)\n",
    "plt.xlabel(\"Predicted Species\")\n",
    "plt.ylabel(\"Actual Species\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a trained model, we can use it to predict the species for a new penguin, based on its measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1613603905524
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "x_new = np.array([[50.4,15.3,224,5550]])\n",
    "print ('New sample: {}'.format(x_new[0]))\n",
    "\n",
    "# The model returns an array of predictions - one for each set of features submitted\n",
    "# In our case, we only submitted one penguin, so our prediction is the first one in the resulting array.\n",
    "penguin_pred = model.predict(x_new)[0]\n",
    "print('Predicted class is', penguin_classes[penguin_pred])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure Machine Learning\n",
    "\n",
    "So far, we've looked at machine learning using fairly common open source frameworks.\n",
    "\n",
    "When you need to work as a team to perform machine learning tasks at scale, Azure Machine Learning can help.\n",
    "\n",
    "One way in which Azure Machine Learning helps is to enable us to manage *experiments*, logging metrics and storing outputs. Commonly, these experiments consist of Python scripts used to train machine learning models - just like we did previously.\n",
    "\n",
    "Let's start by creating a folder to contain the files for our training experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "\n",
    "# Create a folder for the experiment files\n",
    "training_folder = 'penguin-training'\n",
    "os.makedirs(training_folder, exist_ok=True)\n",
    "\n",
    "# Copy the data file into the experiment folder\n",
    "shutil.copy('data/penguin-data.csv', os.path.join(training_folder, \"penguin-data.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create a Python training script, using more or less the same code we used previously; but with the addition of a few Azure Machine Learning commands to track details of the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "%%writefile $training_folder/train-penguins.py\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Import Azure ML run\n",
    "from azureml.core import Run\n",
    "\n",
    "# Get the experiment run context\n",
    "run = Run.get_context()\n",
    "\n",
    "# load the training dataset\n",
    "print('Loading data...')\n",
    "penguins = pd.read_csv('penguin-data.csv')\n",
    "# Drop rows containing NaN values\n",
    "penguins=penguins.dropna()\n",
    "# Separate features and labels\n",
    "penguin_features = ['CulmenLength','CulmenDepth','FlipperLength','BodyMass']\n",
    "penguin_label = 'Species'\n",
    "penguins_X, penguins_y = penguins[penguin_features].values, penguins[penguin_label].values\n",
    "# Split data 70%-30% into training set and test set\n",
    "x_penguin_train, x_penguin_test, y_penguin_train, y_penguin_test = train_test_split(penguins_X, penguins_y,\n",
    "                                                                                    test_size=0.30,\n",
    "                                                                                    random_state=0,\n",
    "                                                                                    stratify=penguins_y)\n",
    "# train a logistic regression model on the training set\n",
    "print('Training model...')\n",
    "reg = 0.1\n",
    "model = LogisticRegression(C=1/reg, solver='lbfgs', multi_class='auto', max_iter=10000).fit(x_penguin_train, y_penguin_train)\n",
    "# Evaluate the model\n",
    "print('Evaluating model')\n",
    "penguin_predictions = model.predict(x_penguin_test)\n",
    "# Log metrics\n",
    "run.log(\"Accuracy\", np.float(accuracy_score(y_penguin_test, penguin_predictions)))\n",
    "run.log(\"Precision\", np.float(precision_score(y_penguin_test, penguin_predictions, average='macro')))\n",
    "run.log(\"Recall\", np.float(recall_score(y_penguin_test, penguin_predictions, average='macro')))\n",
    "# Log the confusion matrix\n",
    "c_matrix = confusion_matrix(y_penguin_test, penguin_predictions)\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "plt.imshow(c_matrix, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
    "plt.colorbar()\n",
    "penguin_classes = ['Amelie', 'Gentoo', 'Chinstrap']\n",
    "tick_marks = np.arange(len(penguin_classes))\n",
    "plt.xticks(tick_marks, penguin_classes, rotation=45)\n",
    "plt.yticks(tick_marks, penguin_classes)\n",
    "plt.xlabel(\"Predicted Species\")\n",
    "plt.ylabel(\"Actual Species\")\n",
    "plt.show()\n",
    "run.log_image(name = 'Confusion Matrix', plot = fig)\n",
    "# Save the trained model in the outputs folder\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "joblib.dump(value=model, filename='outputs/penguin-model.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this experiment, we need to connect to an Azure Machine Learning workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1613605050516
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "\n",
    "# Load the workspace from the saved config file\n",
    "ws = Workspace.from_config()\n",
    "print('Ready to use Azure ML {} to work with {}'.format(azureml.core.VERSION, ws.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can run the experiment - note that I'm using Python code to submit an experiment in which more python code is run. The real magic here though is that I can ask Azure Machine Learning to run the experiment script on remote compute, creating a dedicated Python environment that includes the packages I need, and taking advantage of cloud-scale compute resources that I only pay for while I'm using them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1613607479543
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core import Environment, Experiment, ScriptRunConfig\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "# Create a Python environment for the experiment\n",
    "python_env = Environment(\"penguin-env\")\n",
    "packages = CondaDependencies.create(conda_packages=['scikit-learn','ipykernel','matplotlib','pandas','pip'],\n",
    "                                    pip_packages=['azureml-sdk','pyarrow'])\n",
    "python_env.python.conda_dependencies = packages\n",
    "\n",
    "# Create a script config\n",
    "script_config = ScriptRunConfig(source_directory=training_folder,\n",
    "                                script='train-penguins.py',\n",
    "                                environment=python_env) \n",
    "\n",
    "# submit the experiment\n",
    "experiment_name = 'train-penguins'\n",
    "experiment = Experiment(workspace=ws, name=experiment_name)\n",
    "run = experiment.submit(config=script_config)\n",
    "RunDetails(run).show()\n",
    "run.wait_for_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ater the Experiment has finished, I can go and view its details in [Azure Machine Learning Studio](https://ml.azure.com).\n",
    "\n",
    "One of the outputs from the experiment was a trained model, which I can register in my workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1613607492613
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core import Model\n",
    "\n",
    "# Register the model\n",
    "run.register_model(model_path='outputs/penguin-model.pkl', model_name='penguin_model',\n",
    "                   tags={'Training context':'Ignite Demo'}, properties={'Accuracy': run.get_metrics()['Accuracy']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the model registered, I can deploy it to a web service so that software developers can use it in their applications.\n",
    "\n",
    "(there's code to do that here, but I've already done it to save some time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1613607900721
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from azureml.core.conda_dependencies import CondaDependencies \n",
    "\n",
    "folder_name = 'penguin_service'\n",
    "\n",
    "# Create a folder for the web service files\n",
    "service_folder = './' + folder_name\n",
    "os.makedirs(service_folder, exist_ok=True)\n",
    "\n",
    "print(folder_name, 'folder created.')\n",
    "\n",
    "# Add the dependencies for our model (AzureML defaults is already included)\n",
    "myenv = CondaDependencies()\n",
    "myenv.add_conda_package('scikit-learn')\n",
    "\n",
    "# Save the environment config as a .yml file\n",
    "env_file = os.path.join(service_folder,\"penguin_env.yml\")\n",
    "with open(env_file,\"w\") as f:\n",
    "    f.write(myenv.serialize_to_string())\n",
    "print(\"Saved dependency info in\", env_file)\n",
    "\n",
    "# Print the .yml file\n",
    "with open(env_file,\"r\") as f:\n",
    "    print(f.read())\n",
    "\n",
    "\n",
    "# Set path for scoring script\n",
    "script_file = os.path.join(service_folder,\"score_penguins.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "%%writefile $script_file\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "from azureml.core.model import Model\n",
    "\n",
    "# Called when the service is loaded\n",
    "def init():\n",
    "    global model\n",
    "    # Get the path to the deployed model file and load it\n",
    "    model_path = Model.get_model_path('penguin_model')\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "# Called when a request is received\n",
    "def run(raw_data):\n",
    "    # Get the input data as a numpy array\n",
    "    data = np.array(json.loads(raw_data)['data'])\n",
    "    # Get a prediction from the model\n",
    "    predictions = model.predict(data)\n",
    "    return predictions.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AciWebservice\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core import Model\n",
    "\n",
    "model = ws.models['penguin_model']\n",
    "print(model.name, 'version', model.version)\n",
    "\n",
    "# Configure the scoring environment\n",
    "inference_config = InferenceConfig(runtime= \"python\",\n",
    "                                   entry_script=script_file,\n",
    "                                   conda_file=env_file)\n",
    "\n",
    "deployment_config = AciWebservice.deploy_configuration(cpu_cores = 1, memory_gb = 1)\n",
    "\n",
    "service_name = \"penguin-service\"\n",
    "\n",
    "service = Model.deploy(ws, service_name, [model], inference_config, deployment_config)\n",
    "\n",
    "service.wait_for_deployment(True)\n",
    "print(service.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is deployed to a web service, which has an HTTP endpoint that applications can use to consume it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1613608755087
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core.webservice import Webservice\n",
    "\n",
    "service = Webservice(workspace=ws, name='penguin-service')\n",
    "endpoint = service.scoring_uri\n",
    "print(endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have the endpoint, I can create a simple application that submits some observations of penguins, and gets back a predicted species for each observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1613609038279
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "x_new = [[41.5,18.5,201,4000],\n",
    "         [46.1,13.2,211,4500]]\n",
    "\n",
    "# Convert the array to a serializable list in a JSON document\n",
    "input_json = json.dumps({\"data\": x_new})\n",
    "\n",
    "# Set the content type\n",
    "headers = { 'Content-Type':'application/json' }\n",
    "\n",
    "predictions = requests.post(endpoint, input_json, headers = headers)\n",
    "predicted_classes = predictions.json()\n",
    "penguin_classes = ['Amelie', 'Gentoo', 'Chinstrap']\n",
    "for i in range(len(x_new)):\n",
    "    print (\"Penguin {}\".format(x_new[i]), penguin_classes[predicted_classes[i]] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn More\n",
    "\n",
    "All of the topics discussed in this session, and more, are covered in the Microsoft Azure Data Scientist certification track. You can learn more about the free learning options, and how to get certified at [https://docs.microsoft.com/learn/certifications/azure-data-scientist/](https://docs.microsoft.com/learn/certifications/azure-data-scientist/)"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
